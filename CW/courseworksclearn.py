# -*- coding: utf-8 -*-
"""CourseworkSclearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-tqtx2PovUl7kUidl3qrISnZB29pYhn

**Задание**
* Используя данные из обучающего датасета **housing.csv**, получить обучающий и тестовый наборы , построить модель для предсказания цен на недвижимость (квартиры).
С помощью полученной модели, предсказать цены для квартир из тестового датасета.

* Целевая переменная:
Price

* Метрика качества:
R2 - коэффициент детерминации (sklearn.metrics.r2_score)
* https://github.com/ageron/handson-ml2/tree/master/datasets/housing
* https://www.kaggle.com/harrywang/housing?select=housing.csv

### Подключение и настройка библиотек и скриптов
"""

# Commented out IPython magic to ensure Python compatibility.
# 1. Основные библиотеки
import numpy as np
import pandas as pd
import pickle   # сохранение модели

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# 2. Разделение датасета
from sklearn.model_selection import train_test_split, KFold, GridSearchCV

# 3. Модели
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler

# 4. Метрики качества
from sklearn.metrics import mean_squared_error as mse, r2_score as r2

import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-colorblind')
plt.rcParams['figure.figsize'] = (6, 4)
matplotlib.rcParams.update({'font.size': 14})
pd.set_option('display.float_format', lambda x: '%.2f' % x)
pd.set_option('display.max_rows', 50)

"""### Вспомогательные функции"""

def reduce_mem_usage(df):   # Уменьшить размер датасета
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df

def evaluate_preds(true_values, pred_values, save=False):   # Оценка качества модели и график preds vs true
    
    print("R2:\t" + str(round(r2(true_values, pred_values), 3)) + "\n" +
          "RMSE:\t" + str(round(np.sqrt(mse(true_values, pred_values)), 3)) + "\n" +
          "MSE:\t" + str(round(mse(true_values, pred_values), 3))
         )
    
    plt.figure(figsize=(8,8))
    
    sns.scatterplot(x=pred_values, y=true_values)
    plt.plot([0, 500000], [0, 500000], linestyle='--', color='black')  # диагональ, где true_values = pred_values
    
    plt.xlabel('Predicted values')
    plt.ylabel('True values')
    plt.title('True vs Predicted values')
    
    if save == True:
        plt.savefig(REPORTS_FILE_PATH + 'report.png')
    plt.show()

def show_target_distplot():  # график распределения целевой переменной
  target_mean = round(df['median_house_value'].mean(), 2)
  target_median = df['median_house_value'].median()
  target_mode = df['median_house_value'].mode()[0]
  plt.figure(figsize = (16, 8))

  sns.distplot(df['median_house_value'], bins=50)

  y = np.linspace(0, 0.000005, 10)
  plt.plot([target_mean] * 10, y, label='mean', linestyle=':', linewidth=4)
  plt.plot([target_median] * 10, y, label='median', linestyle='--', linewidth=4)
  plt.plot([target_mode] * 10, y, label='mode', linestyle='-.', linewidth=4)
  plt.title('Distribution of median_house_value')
  plt.legend()
  plt.show()

def get_condition(df, column): # отобрать выбросы 
  max_value = np.quantile(df[column], q=0.975)
  min_value = np.quantile(df[column], q=0.025)
  condition = (df[column] > max_value) | (df[column] < min_value)
  return condition

"""### Загрузка данных

**Пути к директориям и файлам**
"""

from google.colab import drive
drive.mount('/content/gdrive')  # монтируем Google-Drive

DATASET_PATH = 'gdrive/My Drive/Colab Notebooks/housing.csv'
PREPARED_DATASET_PATH = 'gdrive/My Drive/Colab Notebooks/housing_prepared.csv'
SCALER_FILE_PATH = 'gdrive/My Drive/Colab Notebooks/housing_scaler.pkl'
MODEL_FILE_PATH = 'gdrive/My Drive/Colab Notebooks/housing_model.pkl'

"""**Описание датасета**

Статистические данные о ряде домов в Калифорнии, основанные на переписи 1990 года.

* **longitude** - долгота
* **latitude** - широта
* **housing_median_age** - средний возраст дома
* **total_rooms** - общее количество комнат
* **total_bedrooms** - общее количество спален
* **population** - количество проживающих
* **households** - домохозяйства
* **ocean_proximity** - близость океана
* **median_income** - средний доход
* **median_house_value** - средняя стоимость дома  **(target)**

Считываем данные. По строкам - наблюдения, по столбцам - признаки.


"""

df = pd.read_csv(DATASET_PATH, sep=',')
df = reduce_mem_usage(df)  # уменьшим развер DS
df.head()

df.shape

"""### Общая информации о данных"""

df.info()

df.describe().T  # распределение

"""**Распределение целевой переменной**"""

show_target_distplot()

"""Из графика можно увидеть:
* несимметричное распределение целевой переменной;
* странный выброс целевой переменной 500000 с масимальной частотой
* количество строк незначительно

Удалим этот выброс и перестоим график

"""

df = df[df['median_house_value'] <= 500000].reset_index()
show_target_distplot()

"""**Построение нового признака population_per_room и анализ записей DS**"""

df['population_per_room'] = round(df['population'] / df['total_rooms'], 1)
df.loc[df['population_per_room'] < 10, 'population_per_room'].\
    hist(figsize=(4,4), bins=20, grid=False);

df[df['population_per_room'] > 2.5] [['total_rooms', 'population', 'population_per_room']]

df = df[df['population_per_room'] <= 5].reset_index() # отбросим записи, в которых людей в комнате > 5

"""**Категориальные переменные**"""

counts = df['ocean_proximity'].value_counts()
    
plt.figure(figsize=(12,8))    
plt.title('ocean_proximity')
sns.barplot(counts.index, counts.values, log=True)
    
plt.show()

"""**Преобразуем категориальные признаки в бинарные**"""

df.replace(
    {'ocean_proximity':
     {'-': df['ocean_proximity'].mode()[0]}
    },
    inplace=True)
df = pd.concat([df, pd.get_dummies(df['ocean_proximity'])], axis=1)
df.head()

counts = df['NEAR BAY'].value_counts()
    
plt.figure(figsize=(12,8))    
plt.title('NEAR BAY')
sns.barplot(counts.index, counts.values)
    
plt.show()

"""**Количественные переменные**"""

df_num_features = df.select_dtypes(include=['float64', 'float32', 'float16'])
df_num_features.drop('median_house_value', axis=1, inplace=True)
df_num_features.hist(figsize=(16,16), bins=50, grid=False);

"""### Обработка пропусков



"""

df.isna().sum()

"""**housing_median_age**"""

df['housing_median_age'].value_counts()

df['housing_median_age'].fillna(df['housing_median_age'].median(), inplace=True)

"""**total_bedrooms**"""

df['total_bedrooms'].value_counts()

grid = sns.jointplot(df['total_rooms'], df['total_bedrooms'], kind='reg')
grid.fig.set_figwidth(8)
grid.fig.set_figheight(8)
plt.show()

avg_bedrooms_on_avg_rooms = df['total_bedrooms'].mean()/df['total_rooms'].mean()
print(f'Среднее число спален на комнату {avg_bedrooms_on_avg_rooms}')
df.loc[df['total_bedrooms'].isna(), 'total_bedrooms'] = avg_bedrooms_on_avg_rooms * df['total_rooms']

df.isna().sum()

#Компактный вариант с медианой
"""
medians = df[['housing_median_age', 'total_bedrooms', 'population']].median()
print(medians)
df[['housing_median_age', 'total_bedrooms', 'population']] = \
        df[['housing_median_age', 'total_bedrooms', 'population']].fillna(medians)
"""

"""### Обработка выбросов"""

df.describe().T

"""**longitude,  latitude**

Калифорния
Соединённые Штаты Америки
Координаты:
38.561785, -121.449756
"""

df['longitude'].hist(bins=100);
#plt.scatter(df['latitude'], df['longitude'])

df[df['longitude'] >= 0]

df.loc[df['longitude'] > 0, 'longitude'] = df.loc[df['longitude'] > 0, 'longitude'] * -1

df[df['longitude'] == 0]

df.loc[df['longitude'] == 0, 'longitude'] = df['longitude'].median()

df['latitude'].hist(bins=100);

df[(df['latitude'] <= 0) | (df['latitude'] > 50)]

df.loc[(df['latitude'] <= 0) | (df['latitude'] > 50), 'latitude'] = df['latitude'].median()

"""**housing_median_age**"""

df['housing_median_age'].hist(bins=100);

df['housing_median_age'].value_counts()

# df.loc[(df['housing_median_age'] == 28) | (df['housing_median_age'] == 52), 'housing_median_age'] = df['housing_median_age'].mean()

"""Что делать с выбросами 28 и 52 ?
* Присвоение какого-либо значения не решает проблему.
* Механической ошибки быть не может, так как строк много.
* Удалять не стоит, так как это весомая часть датасета.
* Нужен анализ исторических данных, возможно были значительные инвестиции в стоительство.
* Решил оставить как есть.

**total_rooms**
"""

df['total_rooms'].describe()

df['total_rooms'].hist(log=True);

plt.figure(figsize=(6, 3))
sns.boxplot(df['total_rooms'], whis=1.5)
plt.xlabel('total_rooms')
plt.show()

"""Вряд ли в Калифорнии много сверхбольших жилых домов
* Оценка макс числа комнат в жилых домах  ~ 3000 *5 ~15000
* https://domdata.ru/doma-po-kvartiram
"""

df.loc[df['total_rooms'] > 8000, 'total_rooms'] = df['total_rooms'].median()

"""**total_bedrooms**"""

df['total_bedrooms'].describe()

df['total_bedrooms'].hist(log=True);

plt.figure(figsize=(6, 3))
sns.boxplot(df['total_bedrooms'], whis=1.5)
plt.xlabel('total_bedrooms')
plt.show()

df.loc[df['total_bedrooms'] > 1600, 'total_bedrooms'] = df['total_bedrooms'].median()

"""**population**"""

df['population'].describe()

df['population'].hist(log=True);

plt.figure(figsize=(6, 3))
sns.boxplot(df['population'], whis=1.5)
plt.xlabel('population')
plt.show()

df.loc[df['population'] > 10000, 'population'] = df['population'].median()

"""**households**"""

df['households'].describe()

df['households'].hist(log=True);

plt.figure(figsize=(6, 3))
sns.boxplot(df['households'], whis=1.5)
plt.xlabel('households')
plt.show()

df.loc[df['households'] > 2000, 'households'] = df['households'].median()

"""**median_income**"""

df['median_income'].describe()

df['median_income'].hist(log=True);

plt.figure(figsize=(6, 3))
sns.boxplot(df['median_income'], whis=1.5)
plt.xlabel('median_income')
plt.show()

df.loc[df['median_income'] > 8, 'median_income'] = df['median_income'].median()

"""**Оставим признаки, которые необходимы для модели**

"""

df.columns.tolist()

feature_names = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 
                 'households', 'median_income', '<1H OCEAN', 'INLAND', 'NEAR BAY', 'NEAR OCEAN']

target_name = 'median_house_value'
df = df[feature_names + [target_name]]
df.head()

"""**Мультиколлениарность. Матрица корреляций**"""

plt.figure(figsize = (15,10))

sns.set(font_scale=1.4)

corr_matrix = df.corr()
corr_matrix = np.round(corr_matrix, 2)
corr_matrix[np.abs(corr_matrix) < 0.3] = 0

sns.heatmap(corr_matrix, annot=True, linewidths=.5, cmap='coolwarm')

plt.title('Correlation matrix')
plt.show()

"""**Стандартизация признаков**"""

feature_names_for_stand = df[feature_names].select_dtypes(include=['float32', 'float16']).columns.tolist()
scaler = StandardScaler()
stand_features = scaler.fit_transform(df[feature_names_for_stand])
df[feature_names_for_stand] = pd.DataFrame(stand_features, columns=feature_names_for_stand)
df.head()

"""### Сохранение  датасета"""

df.to_csv(PREPARED_DATASET_PATH, index=False, encoding='utf-8', sep=',')

"""### Разбиение на train и test"""

X = df[feature_names]
y = df[target_name]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=42)

"""### Построение базовых моделей и выбор лучшей

**Linear Regression**
"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_train_preds = lr_model.predict(X_train)
evaluate_preds(y_train, y_train_preds)
y_test_preds = lr_model.predict(X_test)
evaluate_preds(y_test, y_test_preds)

"""**Random Forest**"""

rf_model = RandomForestRegressor(max_depth=6)
rf_model.fit(X_train, y_train)
RandomForestRegressor(max_depth=6)
y_train_preds = rf_model.predict(X_train)
evaluate_preds(y_train, y_train_preds)
y_test_preds = rf_model.predict(X_test)
evaluate_preds(y_test, y_test_preds)

"""**Gradient Boosting**

"""

gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
GradientBoostingRegressor()
y_train_preds = gb_model.predict(X_train)
evaluate_preds(y_train, y_train_preds)
y_test_preds = gb_model.predict(X_test)
evaluate_preds(y_test, y_test_preds)

"""### Настройка и оценка финальной модели

**Подбор гиперпараметров**
"""

gb_model = GradientBoostingRegressor(random_state=21)
params = {'n_estimators':[50, 100, 200, 400], 
          'max_depth':[3, 5, 7, 10]}

gs = GridSearchCV(gb_model, params, scoring='r2', cv=KFold(n_splits=3, random_state=21, shuffle=True), n_jobs=-1)
gs.fit(X_train, y_train)
print(gs.best_params_)
print(gs.best_score_)

"""{'max_depth': 5, 'n_estimators': 400}
0.7942140694164087

**Обучение и оценка финальной модели**
"""

final_model = GradientBoostingRegressor(n_estimators=400, max_depth=5, random_state=42)
final_model.fit(X_train, y_train)
y_train_preds = final_model.predict(X_train)
evaluate_preds(y_train, y_train_preds)
y_test_preds = final_model.predict(X_test)
evaluate_preds(y_test, y_test_preds)

"""**Важность признаков**"""

importances = final_model.feature_importances_
feat_importances = pd.Series(importances, index=feature_names)
feat_importances.sort_values(ascending=True, inplace=True)
feat_importances.plot(kind='barh', figsize=(10, 6));

"""### Сохранение модели"""

with open(SCALER_FILE_PATH, 'wb') as file:
    pickle.dump(scaler, file)
with open(MODEL_FILE_PATH, 'wb') as file:
    pickle.dump(final_model, file)